{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97520526-ba5d-4e8e-aa44-22fcc4eaef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import autoencoder\n",
    "from utils.datasets import ImageDataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b677440e-ef63-4153-8708-68a8f3c8a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = autoencoder.Encoder(z_channels=4,\n",
    "                  in_channels=1,\n",
    "                  channels=128,\n",
    "                  channel_multipliers=[1, 2, 4, 4],\n",
    "                  n_resnet_blocks=2)\n",
    "\n",
    "decoder = autoencoder.Decoder(out_channels=1,\n",
    "                  z_channels=4,\n",
    "                  channels=128,\n",
    "                  channel_multipliers=[1, 2, 4, 4],\n",
    "                  n_resnet_blocks=2)\n",
    "\n",
    "ae = autoencoder.Autoencoder(emb_channels=4,\n",
    "                          encoder=encoder,\n",
    "                          decoder=decoder,\n",
    "                          z_channels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb4d01d6-d774-417e-8867-b208d4e3c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(1,1,48,56,48)\n",
    "z= torch.randn(1,4,8,8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96fc9f32-2d87-4d9e-90e4-ade025a9011e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 48, 56, 48])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "821ec40d-79f8-4bd9-96a0-56e5cec2a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size':2,\n",
    "    'epochs':10,\n",
    "    'lr':1e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d14e19cb-90dd-48b2-a6bd-ea4cdf77c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = VAETrainer(ae, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6867624d-aed0-47e8-87b8-2e9c8282fd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset <utils.datasets.ImageDataset object at 0x14f552290>: \n",
      " 10 images.\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data'\n",
    "mode = 'train'\n",
    "dataset = 'dataset_rh_4classes'\n",
    "\n",
    "# Data loader. \n",
    "dataset_file = f'{data_dir}/{mode}-{dataset}.csv'\n",
    "data_flist = pd.read_csv(dataset_file)['filepaths'].iloc[:10]\n",
    "\n",
    "dataset = ImageDataset(\n",
    "    data_flist\n",
    ")\n",
    "\n",
    "print(f'Dataset {dataset}: \\n {len(dataset.data)} images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3a4be7b-ac2e-4640-b17c-f672d69e542b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Start training ----\n",
      "\tEpoch 1 \tAverage Loss:  157.76661109924316\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/latent-style_transfer/vae_trainer.py:62\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     58\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 62\u001b[0m x_hat, posterior \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n\u001b[1;32m     63\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(x, x_hat, posterior\u001b[38;5;241m.\u001b[39mmean, posterior\u001b[38;5;241m.\u001b[39mlog_var)\n\u001b[1;32m     65\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/latent-style_transfer/vae_trainer.py:41\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, x_hat, mean, log_var):\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\t\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\tGet total loss. \u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\tParameters\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\t----------\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m\tx : tensor, source image\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m\tx_hat : tensor, generated image\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m\tmean : float, mean of Gaussian distribution\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\tlog_var : float, log of variance of Gaussian distribution \u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\tReturns \u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m\t-------\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\treproduction_loss + KLD : tensor loss\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;124;03m\t'''\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \tmse \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m     43\u001b[0m \treproduction_loss \u001b[38;5;241m=\u001b[39m mse(x, x_hat)\n",
      "File \u001b[0;32m~/miniforge3/envs/workEnv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/workEnv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "574b98ba-298e-449b-8441-5f5804f80f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 6, 6, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn(1,1,48,56,48)\n",
    "ae.encode(inp).sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c8cccf-7202-4b85-9d47-78726771ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a45d275d-223c-423e-ac2a-54d09d99c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet.UNetModel(in_channels=4,\n",
    "               out_channels=4,\n",
    "               channels=320,\n",
    "               attention_levels=[0, 1, 2],\n",
    "               n_res_blocks=2,\n",
    "               channel_multipliers=[1, 2, 4, 4],\n",
    "               n_heads=8,\n",
    "               tf_layers=1,\n",
    "               d_cond=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "194178b9-7620-4ea5-b276-9ef65f4c75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.randn(1,4,8,8,8)\n",
    "cond = torch.randn(1,1,4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cf78fb6-819d-46c1-80c7-df8d3d9769a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1280, 1, 1, 1])\n",
      "torch.Size([1, 1280, 1, 1, 1])\n",
      "torch.Size([1, 1280, 1, 1, 1])\n",
      "torch.Size([1, 1280, 1, 1, 1])\n",
      "torch.Size([1, 1280, 1, 1, 1])\n",
      "torch.Size([1, 1280, 1, 1, 1])\n",
      "torch.Size([1, 1280, 2, 2, 2])\n",
      "torch.Size([1, 1280, 2, 2, 2])\n",
      "torch.Size([1, 1280, 2, 2, 2])\n",
      "torch.Size([1, 1280, 2, 2, 2])\n",
      "torch.Size([1, 1280, 2, 2, 2])\n",
      "torch.Size([1, 640, 2, 2, 2])\n",
      "torch.Size([1, 1280, 4, 4, 4])\n",
      "torch.Size([1, 640, 4, 4, 4])\n",
      "torch.Size([1, 640, 4, 4, 4])\n",
      "torch.Size([1, 640, 4, 4, 4])\n",
      "torch.Size([1, 640, 4, 4, 4])\n",
      "torch.Size([1, 320, 4, 4, 4])\n",
      "torch.Size([1, 640, 8, 8, 8])\n",
      "torch.Size([1, 320, 8, 8, 8])\n",
      "torch.Size([1, 320, 8, 8, 8])\n",
      "torch.Size([1, 320, 8, 8, 8])\n",
      "torch.Size([1, 320, 8, 8, 8])\n",
      "torch.Size([1, 320, 8, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 8, 8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, torch.tensor([500]), cond).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df380ef3-0b74-427d-b214-7e07a6088eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
